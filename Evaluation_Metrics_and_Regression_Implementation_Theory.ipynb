{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Theoretical :\n",
        "\n",
        "Q1. What does R-squared represent in a regression model?"
      ],
      "metadata": {
        "id": "5H2US1rg8r7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. R-squared is a statistic that shows how well the independent variables in a regression model explain the variability of the dependent variable. It ranges from 0 to 1, where 0 means no explanation of variance and 1 means complete explanation. For example, an R-squared of 0.85 means 85% of the variability in the dependent variable is explained by the model. It's useful for assessing the model's fit, but a higher value doesn't always mean a better model."
      ],
      "metadata": {
        "id": "emokPRNb82Ql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What are the assumptions of linear regression?"
      ],
      "metadata": {
        "id": "0lNsearA890R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. The assumptions of linear regression are:\n",
        "\n",
        "Linearity: The relationship between independent and dependent variables is linear.\n",
        "Independence: Residuals should be independent of each other.\n",
        "Homoscedasticity: Residuals have constant variance across all levels of the independent variable.\n",
        "Normality: Residuals are approximately normally distributed.\n",
        "No multicollinearity: Independent variables should not be highly correlated with each other.\n",
        "These assumptions ensure the model is reliable and results are valid."
      ],
      "metadata": {
        "id": "NNsOf3zI8_gP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. What is the difference between R-squared and Adjusted R-squared?"
      ],
      "metadata": {
        "id": "PbT00fnR9EmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. R-squared measures how much of the variance in the dependent variable is explained by the independent variables, ranging from 0 to 1. Adjusted R-squared also measures goodness of fit but adjusts for the number of predictors in the model. It can decrease if new variables don't improve the model, making it more reliable for comparing models with different numbers of predictors."
      ],
      "metadata": {
        "id": "GBgsuDVq9Hb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Why do we use Mean Squared Error (MSE)?"
      ],
      "metadata": {
        "id": "O4gerXKr9J7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Mean Squared Error (MSE) is used to measure how well a regression model predicts the dependent variable by calculating the average squared differences between predicted and actual values. It is sensitive to outliers, easy to differentiate for optimization, and allows for comparison between different models. A lower MSE indicates a better model fit."
      ],
      "metadata": {
        "id": "IpTcaDo49PbH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What does an Adjusted R-squared value of 0.85 indicate?"
      ],
      "metadata": {
        "id": "3St7tq6M9SsS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. An Adjusted R-squared value of 0.85 indicates that approximately 85% of the variance in the dependent variable can be explained by the independent variables in the model, after adjusting for the number of predictors used. This suggests a strong relationship between the predictors and the outcome, and that the model is likely a good fit for the data. However, it also means that 15% of the variance is not explained by the model."
      ],
      "metadata": {
        "id": "yzIsu1li9jDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. How do we check for normality of residuals in linear regression?"
      ],
      "metadata": {
        "id": "-zg7VXfU9ka9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. To check for normality of residuals in linear regression, you can use:\n",
        "\n",
        "Histogram: Plot a histogram of the residuals; it should look like a bell curve if normally distributed.\n",
        "Q-Q Plot: Create a Q-Q plot; points should fall along a straight line if the residuals are normal.\n",
        "Shapiro-Wilk Test: Perform this test; a high p-value (above 0.05) suggests normality.\n",
        "Skewness and Kurtosis: Check that skewness is close to 0 and kurtosis is close to 3 for normal distribution.These methods help determine if the residuals are normally distributed"
      ],
      "metadata": {
        "id": "UJDZh2X29sug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. What is multicollinearity, and how does it impact regression?"
      ],
      "metadata": {
        "id": "MMVwlFN4ATCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Multicollinearity is when two or more independent variables in a regression model are highly correlated. This can lead to:\n",
        "\n",
        "Inflated standard errors, making it hard to identify significant predictors.\n",
        "Unstable coefficients that can change with small data variations.\n",
        "Difficulty in interpreting the individual effects of correlated variables.\n",
        "Reduced model performance and predictive power.\n",
        "To detect it, you can use variance inflation factors (VIF) or correlation matrices. If found, consider removing or combining correlated variables to improve the model."
      ],
      "metadata": {
        "id": "6Auf-J1ZAUOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What is Mean Absolute Error (MAE)?"
      ],
      "metadata": {
        "id": "Oz1tqU2qAZWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Mean Absolute Error (MAE) is a metric that measures the average absolute differences between predicted values and actual values in a model. To calculate it, you take the absolute errors for each observation, sum them up, and then divide by the total number of observations.\n",
        "\n",
        "The formula is:\n",
        "\n",
        "MAE = (1/n) * Σ |actual - predicted|\n",
        "\n",
        "MAE provides a clear indication of the average error in the same units as the data, with a lower MAE indicating better model accuracy."
      ],
      "metadata": {
        "id": "KmDfYjQ2AcSh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What are the benefits of using an ML pipeline?"
      ],
      "metadata": {
        "id": "DQDu2A2fAdey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Using an ML pipeline has several benefits:\n",
        "\n",
        "Efficiency: Automates the workflow, saving time.\n",
        "Reproducibility: Ensures consistent results across runs.\n",
        "Modularity: Allows easy swapping of components.\n",
        "Scalability: Handles larger datasets effectively.\n",
        "Improved Collaboration: Facilitates teamwork on projects.\n",
        "Error Reduction: Minimizes human errors in processes.\n",
        "Overall, it streamlines the machine learning process and improves quality."
      ],
      "metadata": {
        "id": "hb4x__PsAfvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. Why is RMSE considered more interpretable than MSE?"
      ],
      "metadata": {
        "id": "RAVdXpKDAkQG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. RMSE (Root Mean Square Error) is more interpretable than MSE (Mean Squared Error) because:\n",
        "\n",
        "Units: RMSE is in the same units as the original data, making it easier to understand.\n",
        "Interpretability: It shows the average error directly, which is more intuitive.\n",
        "Sensitivity to Outliers: RMSE clearly indicates the impact of outliers on the error.\n",
        "Overall, RMSE provides a clearer measure of model accuracy."
      ],
      "metadata": {
        "id": "jeRm-49vAnHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. What is pickling in Python, and how is it useful in ML?"
      ],
      "metadata": {
        "id": "MYhmy_ihAp11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Pickling in Python is the process of converting Python objects into a byte stream for saving or transmitting. It's useful in machine learning because:\n",
        "\n",
        "Model Persistence: You can save trained models and load them later without retraining.\n",
        "Efficiency: Saves time and resources by reusing models.\n",
        "Deployment: Easily share or deploy models without the training data.\n",
        "Version Control: Helps manage different versions of models.\n",
        "Overall, it streamlines the workflow in ML projects"
      ],
      "metadata": {
        "id": "1AdNTALnAtAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. What does a high R-squared value mean?"
      ],
      "metadata": {
        "id": "vCigFLAlAwbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. A high R-squared value means that a large portion of the variance in the dependent variable is explained by the independent variables in the model. It indicates a good fit, suggesting that the model predictions are close to actual values. However, it doesn't guarantee that the model is the best or that all predictors are significant."
      ],
      "metadata": {
        "id": "wyejbu_9Az2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. What happens if linear regression assumptions are violated?"
      ],
      "metadata": {
        "id": "jS7eNMs_A19v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. If linear regression assumptions are violated, several issues can arise:\n",
        "\n",
        "Bias: Predictions may be biased, leading to inaccurate results.\n",
        "Inefficiency: The estimates of coefficients may become inefficient, increasing the standard errors.\n",
        "Invalid Inferences: Hypothesis tests may produce unreliable results, affecting p-values and confidence intervals.\n",
        "Poor Predictions: The model's predictive power may decrease, leading to poor performance on new data.\n",
        "Overall, violating assumptions can compromise the validity and reliability of the regression analysis."
      ],
      "metadata": {
        "id": "t4kE3s5RA4BP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. How can we address multicollinearity in regression?"
      ],
      "metadata": {
        "id": "2231ZhSPA-lf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. To address multicollinearity in regression, you can:\n",
        "\n",
        "Remove one of the correlated variables.\n",
        "Combine correlated variables into a single predictor.\n",
        "Use Principal Component Analysis (PCA) to create uncorrelated variables.\n",
        "Apply regularization techniques like Ridge or Lasso regression.\n",
        "Increase the sample size if possible.\n",
        "These steps can help improve the reliability of your regression model."
      ],
      "metadata": {
        "id": "w21-GWK9BaRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15. How can feature selection improve model performance in regression analysis?"
      ],
      "metadata": {
        "id": "pDff21zLBbWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Feature selection can improve model performance in regression analysis by:\n",
        "\n",
        "Reducing overfitting, leading to better generalization.\n",
        "Enhancing interpretability, making the model easier to understand.\n",
        "Decreasing training time by processing fewer features.\n",
        "Increasing accuracy by focusing on the most relevant features.\n",
        "Overall, it helps create more efficient and effective models.\n",
        "\n"
      ],
      "metadata": {
        "id": "EeXqeAYjBepf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. How is Adjusted R-squared calculated?"
      ],
      "metadata": {
        "id": "SZh1mGsCBhpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Adjusted R-squared is calculated using the formula:\n",
        "\n",
        "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - p - 1)]\n",
        "\n",
        "Here, R-squared is the original R-squared, n is the number of observations, and p is the number of independent variables. It adjusts the R-squared value based on the number of predictors, providing a more accurate measure of model performance."
      ],
      "metadata": {
        "id": "6wCIGDyfBkqi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17. Why is MSE sensitive to outliers?"
      ],
      "metadata": {
        "id": "mLK3_jnxBoB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. MSE is sensitive to outliers because it squares the differences between predicted and actual values. When there's an outlier, the squared difference becomes very large, which significantly increases the MSE. This can distort the overall error measurement and lead to misleading conclusions about the model's accuracy."
      ],
      "metadata": {
        "id": "4TNPVIpFBszO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18. What is the role of homoscedasticity in linear regression?"
      ],
      "metadata": {
        "id": "SlAbtdlFBuTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Homoscedasticity means that the variance of errors in a linear regression model is constant across all levels of the independent variable(s). Its role is crucial because:\n",
        "\n",
        "It is a key assumption of linear regression.\n",
        "It ensures that OLS estimates are efficient and unbiased.\n",
        "It allows for reliable hypothesis testing and accurate standard errors.\n",
        "If homoscedasticity is violated, it can lead to inefficient estimates and unreliable results."
      ],
      "metadata": {
        "id": "vM45LfQvBwa-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19. What is Root Mean Squared Error (RMSE)?"
      ],
      "metadata": {
        "id": "ZTsYnI9uB0qi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Root Mean Squared Error (RMSE) is a metric that measures the average differences between predicted and actual values in a regression model. It is calculated by taking the square root of the average of the squared errors. A lower RMSE indicates a better fit of the model, and it's in the same units as the original data, making it easy to interpret."
      ],
      "metadata": {
        "id": "dk7SeYq3B2yo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20. Why is pickling considered risky?"
      ],
      "metadata": {
        "id": "JnfyCbz_B65U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Pickling is considered risky because unpickling data can execute malicious code if the data comes from an untrusted source. It can also lead to errors if there are changes in class definitions or the environment. Therefore, it's essential to only unpickle data from trusted sources"
      ],
      "metadata": {
        "id": "ifpMkRkYB9ei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q21. What alternatives exist to pickling for saving ML models?"
      ],
      "metadata": {
        "id": "pe9thImHCDYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Alternatives to pickling for saving ML models include:\n",
        "\n",
        "Joblib: Efficient for large numpy arrays and easy to use.\n",
        "JSON: Good for saving model parameters and architecture, but not for complex objects.\n",
        "HDF5: Suitable for large datasets and hierarchical data.\n",
        "ONNX: A format for interoperability between different ML frameworks.\n",
        "TensorFlow SavedModel: For TensorFlow models, preserving the entire model architecture and weights.\n",
        "These methods can enhance security and compatibility compared to pickling."
      ],
      "metadata": {
        "id": "idw8BXt_CJHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q22. What is heteroscedasticity, and why is it a problem?"
      ],
      "metadata": {
        "id": "Nfbi7KL9CKWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Heteroscedasticity is when the variance of errors in a regression model is not constant across all levels of the independent variable. It’s a problem because it violates the assumptions of ordinary least squares regression, leading to inefficient estimates and biased standard errors, which can affect the reliability of hypothesis tests and confidence intervals."
      ],
      "metadata": {
        "id": "FZqxsD4oCNfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q23. How can interaction terms enhance a regression model's predictive power?"
      ],
      "metadata": {
        "id": "Fc72P5cNCPnu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Interaction terms can enhance a regression model's predictive power by capturing the combined effects of two or more independent variables on the dependent variable. They allow the model to show how the relationship between one predictor and the outcome changes based on the level of another predictor, leading to more accurate predictions and insights into complex relationships."
      ],
      "metadata": {
        "id": "5LToYH78CStM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GhRxelT3CE9W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}